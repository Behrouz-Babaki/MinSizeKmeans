{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l2_distance(point1, point2):\n",
    "    return sum([(float(i)-float(j))**2 for (i,j) in zip(point1, point2)])\n",
    "\n",
    "class subproblem(object):\n",
    "    def __init__(self, centroids, data, min_size):\n",
    "        \n",
    "        self.centroids = centroids\n",
    "        self.data = data\n",
    "        self.min_size = min_size\n",
    "        self.n = len(data)\n",
    "        self.k = len(centroids)\n",
    "        \n",
    "        self.create_model()\n",
    "               \n",
    "    def create_model(self):\n",
    "        def distances(assignment):\n",
    "            return l2_distance(self.data[assignment[0]], self.centroids[assignment[1]])\n",
    "        \n",
    "        clusters = list(range(self.k))\n",
    "        assignments = [(i, j)for i in range(self.n) for j in range(self.k)]\n",
    "        \n",
    "        # outflow variables for data nodes\n",
    "        self.y = pulp.LpVariable.dicts('data-to-cluster assignments', \n",
    "                                  assignments, \n",
    "                                  lowBound=0,\n",
    "                                  upBound=1,\n",
    "                                  cat=pulp.LpInteger)\n",
    "\n",
    "        # outflow variables for cluster nodes\n",
    "        self.b = pulp.LpVariable.dicts('cluster outflows',\n",
    "                                  clusters,\n",
    "                                  lowBound=0,\n",
    "                                  upBound=self.n-self.min_size,\n",
    "                                  cat=pulp.LpContinuous)\n",
    "        \n",
    "        # create the model\n",
    "        self.model = pulp.LpProblem(\"Model for assignment subproblem\", pulp.LpMinimize)\n",
    "        \n",
    "        # objective function\n",
    "        self.model += sum([distances(assignment) * self.y[assignment] for assignment in assignments])\n",
    "        \n",
    "        # flow balance constraints for data nodes\n",
    "        for i in range(self.n):\n",
    "            self.model += sum(self.y[(i, j)] for j in range(self.k)) == 1\n",
    "        \n",
    "        # flow balance constraints for cluster nodes\n",
    "        for j in range(k):\n",
    "            self.model += sum(self.y[(i, j)] for i in range(self.n)) - self.min_size == self.b[j]    \n",
    "\n",
    "        # flow balance constraint for the sink node\n",
    "        self.model += sum(self.b[j] for j in range(self.k)) == self.n - (self.k * min_size)\n",
    "        \n",
    "    \n",
    "    def solve(self):\n",
    "        self.status = self.model.solve()\n",
    "        \n",
    "        clusters = None\n",
    "        if self.status == 1:\n",
    "            clusters= [-1 for i in range(self.n)]\n",
    "            for i in range(self.n):\n",
    "                for j in range(self.k):\n",
    "                    if self.y[(i, j)].value() > 0:\n",
    "                        clusters[i] = j\n",
    "        return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def initialize_centers(dataset, k):\n",
    "    ids = list(range(len(dataset)))\n",
    "    random.shuffle(ids)\n",
    "    return [dataset[id] for id in ids[:k]]\n",
    "\n",
    "def compute_centers(clusters, dataset):\n",
    "    # canonical labeling of clusters\n",
    "    ids = list(set(clusters))\n",
    "    c_to_id = dict()\n",
    "    for j, c in enumerate(ids):\n",
    "        c_to_id[c] = j\n",
    "    for j, c in enumerate(clusters):\n",
    "        clusters[j] = c_to_id[c]\n",
    "    \n",
    "    k = len(ids)\n",
    "    dim = len(dataset[0])\n",
    "    centers = [[0.0] * dim for i in range(k)]\n",
    "    counts = [0] * k\n",
    "    for j, c in enumerate(clusters):\n",
    "        for i in range(dim):\n",
    "            centers[c][i] += dataset[j][i]\n",
    "        counts[c] += 1\n",
    "    for j in range(k):\n",
    "        for i in range(dim):\n",
    "            centers[j][i] = centers[j][i]/float(counts[j])\n",
    "    return clusters, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minsize_kmeans (dataset, k, min_size=0):\n",
    "    \n",
    "    centers = initialize_centers(dataset, k)\n",
    "    clusters = [-1] * len(dataset)\n",
    "    \n",
    "    converged = False\n",
    "    while not converged:\n",
    "        m = subproblem(centers, data, min_size)\n",
    "        clusters_ = m.solve()\n",
    "        if not clusters_:\n",
    "            return None, None\n",
    "        clusters_, centers = compute_centers(clusters_, dataset)\n",
    "        \n",
    "        converged = True\n",
    "        i = 0\n",
    "        while converged and i < len(dataset):\n",
    "            if clusters[i] != clusters_[i]:\n",
    "                converged = False\n",
    "            i += 1\n",
    "        clusters = clusters_\n",
    "        \n",
    "    return clusters, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(datafile):\n",
    "    data = []\n",
    "    with open(datafile, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                d = [float(i) for i in line.split()]\n",
    "                data.append(d)\n",
    "    return data\n",
    "\n",
    "def cluster_quality(cluster):\n",
    "    if len(cluster) == 0:\n",
    "        return 0.0\n",
    "       \n",
    "    quality = 0.0\n",
    "    for i in range(len(cluster)):\n",
    "        for j in range(i, len(cluster)):\n",
    "            quality += l2_distance(cluster[i], cluster[j])\n",
    "    return quality / len(cluster)\n",
    "    \n",
    "def compute_quality(data, cluster_indices):\n",
    "    clusters = dict()\n",
    "    for i, c in enumerate(cluster_indices):\n",
    "        if c in clusters:\n",
    "            clusters[c].append(data[i])\n",
    "        else:\n",
    "            clusters[c] = [data[i]]\n",
    "    return sum(cluster_quality(c) for c in clusters.values())\n",
    "\n",
    "data = read_data('../data/iris.data')\n",
    "k = 3\n",
    "min_size = 1\n",
    "clusters, centers = minsize_kmeans(data, k, min_size)\n",
    "print('%.4f'%compute_quality(data, clusters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
